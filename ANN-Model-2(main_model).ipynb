{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B8LNTtGQhqqgmc9HGEPzMuMWfYFvLzd8","timestamp":1683249934441}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gnuWNbIkcHpa"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from tqdm.notebook import tqdm, trange\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")"],"metadata":{"id":"wiQEde1KnIx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_dir = '/content/drive/MyDrive/Project/us-patent-phrase-to-phrase-matching'\n","train_path = data_dir + '/train_data.csv'\n","val_path = data_dir + '/val_data.csv'\n","test_path = data_dir + '/test_data.csv'"],"metadata":{"id":"vvRVR36pp3hS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PatentDataset(Dataset):\n","    def process_data(self, data_df):\n","        score_to_class = {0: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1: 4}\n","        data_df['class'] = data_df.apply(lambda x: score_to_class[x['score']], axis=1)\n","\n","        y = data_df['class'].to_numpy()\n","        data_df = data_df.drop(columns=['score', 'class'])\n","        #print(list(data_df.columns))\n","        anchor_cols = ['anchor_' + str(i) for i in range(768)]\n","        target_cols = ['target_' + str(i) for i in range(768)]\n","        anchor_data = data_df[anchor_cols].to_numpy()\n","        #print('anchor_data', anchor_data)\n","        #print('anchor_data', anchor_data.shape)\n","        target_data = data_df[target_cols].to_numpy()\n","        #print('target_data', target_data)\n","        #print('target_data', target_data.shape)\n","        aggr_data = np.multiply(anchor_data, target_data)\n","        #print('aggr_data', aggr_data)\n","        #print('aggr_data', aggr_data.shape)\n","        data_df = data_df.drop(columns=anchor_cols)\n","        data_df = data_df.drop(columns=target_cols)\n","        #print('data_df', data_df.shape)\n","        #print('data_df', list(data_df.columns))\n","        context_data = data_df.to_numpy()\n","        #print('context_data', context_data.shape)\n","        X = np.concatenate((aggr_data, context_data), axis=1)\n","        #print(X.shape, y.shape)\n","        return X, y\n","    def __init__(self, data_path):\n","        super(PatentDataset, self).__init__()\n","\n","        data_df = pd.read_csv(data_path)\n","        X, y = self.process_data(data_df)\n","\n","        self.X = torch.from_numpy(X).double()\n","        self.y = torch.from_numpy(y).long()\n","        print(self.X.dtype)\n","        print('X.shape', self.X.shape, 'y.shape', self.y.shape)\n","        self.len = X.shape[0]\n","\n","    def __getitem__(self, index):\n","        if torch.is_tensor(index):\n","           index = index.tolist()\n","        return self.X[index], self.y[index] \n","\n","    def __len__(self):\n","        return self.len"],"metadata":{"id":"GW3AuyuZnP0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_set = PatentDataset(train_path)\n","val_set = PatentDataset(val_path)\n","test_set = PatentDataset(test_path)"],"metadata":{"id":"JMN9CLyypOCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n","val_loader = DataLoader(dataset=val_set, batch_size=32, shuffle=False)\n","test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=False)"],"metadata":{"id":"4ZwdqnPcpYQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.fc1 = nn.Linear(874, 400) \n","        self.fc2 = nn.Linear(400, 600)\n","        self.fc3 = nn.Linear(600, 800)\n","        self.fc4 = nn.Linear(800, 500)\n","        self.fc5 = nn.Linear(500, 200)  \n","        self.fc6 = nn.Linear(200,5)\n","        print(self.fc2.weight.dtype, self.fc2.bias.dtype, self.fc3.bias.dtype, self.fc4.bias.dtype, self.fc5.bias.dtype, self.fc6.bias.dtype)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.relu(self.fc4(x))\n","        x = F.relu(self.fc5(x))\n","        x = self.fc6(x)\n","        return x"],"metadata":{"id":"Fy1PlF12hmf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model().double().to(device)\n","print(sum(p.numel() for p in model.parameters()))"],"metadata":{"id":"srPTCbfGjapp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_loss(loss, title, xlabel, ylabel):\n","    epoch = list(range(len(loss)))\n","    plt.plot(epoch, loss)\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.show()"],"metadata":{"id":"evEFIkgSt2Jj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"5_OU9nlQjqEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch_count = 100\n","epoch_train_loss = []\n","epoch_val_loss = []\n","patience = np.Inf\n","patience_lost = 0\n","for epoch in range(1, epoch_count+1):\n","    print('Epoch', epoch)\n","    model.train()\n","    batch_train_loss = []\n","    for (X, y) in tqdm(train_loader, desc='Training epoch ' + str(epoch), leave=False):\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        pred = model(X)\n","        loss = criterion(pred, y)\n","        loss.backward()\n","        optimizer.step()\n","        batch_train_loss.append(loss.item())\n","    epoch_train_loss.append(np.mean(batch_train_loss))\n","    print('Train loss: %.3f' % epoch_train_loss[-1], flush=True, end='')\n","\n","    batch_val_loss = []\n","    model.eval()\n","    with torch.no_grad():\n","        for (X, y) in tqdm(val_loader, desc='Validation epoch', leave=False):\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","            loss = criterion(pred, y)\n","            batch_val_loss.append(loss.item())\n","    epoch_val_loss.append(np.mean(batch_val_loss))\n","    print('Val loss: %.3f' % epoch_val_loss[-1], flush=True)\n","    if(epoch > 1):\n","        if(epoch_val_loss[-1] >= epoch_val_loss[-2]):\n","            patience_lost += 1\n","            if(patience_lost == patience):\n","                break\n","        else:\n","            patience_lost = 0        "],"metadata":{"id":"GFzotdEEptIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_loss(epoch_train_loss, 'Train cross entropy loss across epochs', 'Epoch', 'Train CrossEntropyLoss')\n","plot_loss(epoch_val_loss, 'Validation cross entropy loss across epochs', 'Epoch', 'Val CrossEntropyLoss')"],"metadata":{"id":"voKApm7Dt3fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.special import softmax\n","from sklearn.metrics import classification_report\n","\n","def calc_metrics(dataset, model):\n","    y_true = None\n","    y_pred = None\n","    with torch.no_grad():\n","        X, y = dataset.X.to(device), dataset.y.to(device)\n","        pred = model(X)\n","        y_true = y.cpu().detach().numpy()\n","        y_pred = pred.cpu().detach().numpy()\n","    y_pred = softmax(y_pred, axis=1)\n","    y_pred = np.argmax(y_pred, axis=1)\n","    report = classification_report(y_true, y_pred)\n","    print(report)"],"metadata":{"id":"Vpuzj7sVslm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Train evaluation')\n","calc_metrics(train_set, model)\n","print('Validation evaluation')\n","calc_metrics(val_set, model)\n","print('Test evaluation')\n","calc_metrics(test_set, model)"],"metadata":{"id":"Ho5dXAquNm-L"},"execution_count":null,"outputs":[]}]}